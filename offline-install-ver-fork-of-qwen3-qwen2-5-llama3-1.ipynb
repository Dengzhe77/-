{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7401b0b",
   "metadata": {
    "papermill": {
     "duration": 0.003645,
     "end_time": "2025-09-08T07:22:27.061810",
     "exception": false,
     "start_time": "2025-09-08T07:22:27.058165",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### References\n",
    "\n",
    "*   [https://www.kaggle.com/code/abdmental01/jigsaw-mpnet-base-v2-inference-cv-0-876](https://www.kaggle.com/code/abdmental01/jigsaw-mpnet-base-v2-inference-cv-0-876)\n",
    "*   [https://www.kaggle.com/code/aerdem4/jigsaw-acrc-qwen7b-finetune-logits-processor-zoo](https://www.kaggle.com/code/aerdem4/jigsaw-acrc-qwen7b-finetune-logits-processor-zoo)\n",
    "*   [https://www.guruguru.science/competitions/24/discussions/21027ff1-2074-4e21-a249-b2d4170bd516/](https://www.guruguru.science/competitions/24/discussions/21027ff1-2074-4e21-a249-b2d4170bd516/)\n",
    "*   https://www.kaggle.com/code/mks2192/jigsaw-llama3-1-8b-instruct-training-one-epoch\n",
    "*   [https://www.kaggle.com/code/fuumin621/qwen2-5-lora-finetune-baseline-inference](https://www.kaggle.com/code/fuumin621/qwen2-5-lora-finetune-baseline-inference)\n",
    "*   https://www.kaggle.com/code/neibyr/30-min-just-use-semantic-search-qwen3-emb-0-6b\n",
    "*   https://www.kaggle.com/code/bibanh/qwen3-embedding-qwen2-5-32b-llama3-1-8b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff41a72",
   "metadata": {
    "papermill": {
     "duration": 0.002564,
     "end_time": "2025-09-08T07:22:27.067404",
     "exception": false,
     "start_time": "2025-09-08T07:22:27.064840",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### I want to say thanks to @neibyr for your interesting idea: [Retrieve by Qwen3Embedding](http://https://www.kaggle.com/code/neibyr/30-min-just-use-semantic-search-qwen3-emb-0-6b) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dde2aa",
   "metadata": {
    "papermill": {
     "duration": 0.002529,
     "end_time": "2025-09-08T07:22:27.072730",
     "exception": false,
     "start_time": "2025-09-08T07:22:27.070201",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# diff\n",
    "+ Offline library install by uv [Reason: Dependencies's workflow switching internet on-off is tiresome]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e936f321",
   "metadata": {
    "papermill": {
     "duration": 0.002524,
     "end_time": "2025-09-08T07:22:27.077964",
     "exception": false,
     "start_time": "2025-09-08T07:22:27.075440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Qwen2.5 32B GPTQ Int4 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f246909a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T07:22:27.084514Z",
     "iopub.status.busy": "2025-09-08T07:22:27.084266Z",
     "iopub.status.idle": "2025-09-08T07:22:27.721488Z",
     "shell.execute_reply": "2025-09-08T07:22:27.720736Z"
    },
    "papermill": {
     "duration": 0.642297,
     "end_time": "2025-09-08T07:22:27.723043",
     "exception": false,
     "start_time": "2025-09-08T07:22:27.080746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\r\n",
      "\u001b[2K  \u001b[31m×\u001b[0m No solution found when resolving dependencies:\r\n",
      "\u001b[31m  ╰─▶ \u001b[0mBecause torch==2.8.0 depends on triton{platform_machine == 'x86_64' and\r\n",
      "\u001b[31m      \u001b[0msys_platform == 'linux'}==3.4.0 and only the following versions of torch\r\n",
      "\u001b[31m      \u001b[0mare available:\r\n",
      "\u001b[31m      \u001b[0m    torch==2.7.1\r\n",
      "\u001b[31m      \u001b[0m    torch==2.8.0\r\n",
      "\u001b[31m      \u001b[0mwe can conclude that torch>2.7.1 depends on triton==3.4.0. (1)\r\n",
      "\r\n",
      "\u001b[31m      \u001b[0mBecause there is no version of nvidia-cuda-nvrtc-cu12{platform_machine\r\n",
      "\u001b[31m      \u001b[0m== 'x86_64' and sys_platform == 'linux'}==12.4.127 and\r\n",
      "\u001b[31m      \u001b[0mtorch==2.6.0+cu124 depends on nvidia-cuda-nvrtc-cu12{platform_machine\r\n",
      "\u001b[31m      \u001b[0m== 'x86_64' and sys_platform == 'linux'}==12.4.127, we can conclude that\r\n",
      "\u001b[31m      \u001b[0mtorch==2.6.0+cu124 cannot be used.\r\n",
      "\u001b[31m      \u001b[0mAnd because we know from (1) that torch>2.7.1 depends on triton==3.4.0,\r\n",
      "\u001b[31m      \u001b[0mwe can conclude that torch>2.7.1 depends on triton==3.4.0.\r\n",
      "\u001b[31m      \u001b[0mAnd because torch==2.7.1 depends on triton{platform_machine == 'x86_64'\r\n",
      "\u001b[31m      \u001b[0mand sys_platform == 'linux'}==3.3.1 and logits-processor-zoo==0.1.10\r\n",
      "\u001b[31m      \u001b[0mdepends on torch, we can conclude that logits-processor-zoo==0.1.10\r\n",
      "\u001b[31m      \u001b[0mdepends on triton>=3.3.1.\r\n",
      "\u001b[31m      \u001b[0mAnd because you require logits-processor-zoo==0.1.10 and triton==3.2.0,\r\n",
      "\u001b[31m      \u001b[0mwe can conclude that your requirements are unsatisfiable.\r\n"
     ]
    }
   ],
   "source": [
    "#这个版本就是离线下载版的程序这是为了避免当进行测试时kaggle官方会断网\n",
    "!uv pip install -U --system --no-index --find-links='/kaggle/input/jigsaw-packages/whls/' 'vllm' 'logits-processor-zoo==0.1.10' 'triton==3.2.0' 'clean-text' 'bitsandbytes' 'peft' 'accelerate' 'datasets' 'emoji' 'setuptools>=40.8.0' 'numpy<2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20a4cb5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T07:22:27.731224Z",
     "iopub.status.busy": "2025-09-08T07:22:27.730570Z",
     "iopub.status.idle": "2025-09-08T07:22:27.849084Z",
     "shell.execute_reply": "2025-09-08T07:22:27.848172Z"
    },
    "papermill": {
     "duration": 0.123753,
     "end_time": "2025-09-08T07:22:27.850198",
     "exception": false,
     "start_time": "2025-09-08T07:22:27.726445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! mkdir -p /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfbe4f23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T07:22:27.857493Z",
     "iopub.status.busy": "2025-09-08T07:22:27.857261Z",
     "iopub.status.idle": "2025-09-08T07:22:27.864068Z",
     "shell.execute_reply": "2025-09-08T07:22:27.863482Z"
    },
    "papermill": {
     "duration": 0.011703,
     "end_time": "2025-09-08T07:22:27.865091",
     "exception": false,
     "start_time": "2025-09-08T07:22:27.853388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/infer_qwen.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/infer_qwen.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "import torch\n",
    "import vllm\n",
    "import numpy as np\n",
    "from vllm.lora.request import LoRARequest\n",
    "import argparse\n",
    "from scipy.special import softmax\n",
    "df = pd.read_csv(\"/kaggle/input/jigsaw-agile-community-rules/test.csv\")\n",
    "\n",
    "MODEL_NAME = \"/kaggle/input/qwen2-5-32b-instruct-gptq-int4\"\n",
    "LORA_PATH = \"/kaggle/input/qwen2-5-32b-gptq-int4-batch4-full\"\n",
    "if __name__=='__main__':\n",
    "    os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "\n",
    "    llm = vllm.LLM(\n",
    "        MODEL_NAME,\n",
    "        # quantization='awq',\n",
    "        quantization='gptq',\n",
    "        tensor_parallel_size=torch.cuda.device_count(),\n",
    "        gpu_memory_utilization=0.95,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"half\",\n",
    "        enforce_eager=True,\n",
    "        max_model_len=4096,\n",
    "        disable_log_stats=True,\n",
    "        enable_prefix_caching=True,\n",
    "        enable_lora=True,\n",
    "    )\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    SYS_PROMPT = \"\"\"\n",
    "    You are given a comment on reddit. Your task is to classify if it violates the given rule. Only respond Yes/No.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompts = []\n",
    "    for i, row in df.iterrows():\n",
    "        text = f\"\"\"\n",
    "    r/{row.subreddit}\n",
    "    Rule: {row.rule}\n",
    "    \n",
    "    1) {row.positive_example_1}\n",
    "    Violation: Yes\n",
    "    \n",
    "    2) {row.positive_example_2}\n",
    "    Violation: Yes\n",
    "    \n",
    "    3) {row.negative_example_1}\n",
    "    Violation: No\n",
    "    \n",
    "    4) {row.negative_example_2}\n",
    "    Violation: No\n",
    "    \n",
    "    5) {row.body}\n",
    "    \"\"\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ]\n",
    "    \n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False,\n",
    "        ) + \"Answer:\"\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    df[\"prompt\"] = prompts\n",
    "    \n",
    "    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=['Yes','No'])\n",
    "    outputs = llm.generate(\n",
    "        prompts,\n",
    "        vllm.SamplingParams(\n",
    "            skip_special_tokens=True,\n",
    "            max_tokens=1,\n",
    "            logits_processors=[mclp],\n",
    "            logprobs=2,\n",
    "        ),\n",
    "        use_tqdm=True,\n",
    "        lora_request=LoRARequest(\"default\", 1, LORA_PATH)\n",
    "    )\n",
    "    logprobs = [\n",
    "        {lp.decoded_token: lp.logprob for lp in out.outputs[0].logprobs[0].values()}\n",
    "        for out in outputs\n",
    "    ]\n",
    "    logit_matrix = pd.DataFrame(logprobs)[['Yes','No']]\n",
    "    df = pd.concat([df, logit_matrix], axis=1)\n",
    "    \n",
    "    df[['Yes',\"No\"]] = df[['Yes',\"No\"]].apply(lambda x: softmax(x.values), axis=1, result_type=\"expand\")\n",
    "    df[\"pred\"] = df[\"Yes\"]\n",
    "    df['rule_violation'] = df[\"pred\"]\n",
    "    df[['row_id', 'rule_violation']].to_csv(\"submission_qwen.csv\",index=False)\n",
    "    pd.read_csv('submission_qwen.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d70b1217",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T07:22:27.872173Z",
     "iopub.status.busy": "2025-09-08T07:22:27.871970Z",
     "iopub.status.idle": "2025-09-08T07:22:29.731226Z",
     "shell.execute_reply": "2025-09-08T07:22:29.730139Z"
    },
    "papermill": {
     "duration": 1.864478,
     "end_time": "2025-09-08T07:22:29.732762",
     "exception": false,
     "start_time": "2025-09-08T07:22:27.868284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/tmp/infer_qwen.py\", line 4, in <module>\r\n",
      "    from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\r\n",
      "ModuleNotFoundError: No module named 'logits_processor_zoo'\r\n"
     ]
    }
   ],
   "source": [
    "%cd /tmp\n",
    "!python infer_qwen.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5286383",
   "metadata": {
    "papermill": {
     "duration": 0.003154,
     "end_time": "2025-09-08T07:22:29.739553",
     "exception": false,
     "start_time": "2025-09-08T07:22:29.736399",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Qwen3 0.6b Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fff8e02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T07:22:29.747958Z",
     "iopub.status.busy": "2025-09-08T07:22:29.747723Z",
     "iopub.status.idle": "2025-09-08T07:22:29.753224Z",
     "shell.execute_reply": "2025-09-08T07:22:29.752359Z"
    },
    "papermill": {
     "duration": 0.010712,
     "end_time": "2025-09-08T07:22:29.754346",
     "exception": false,
     "start_time": "2025-09-08T07:22:29.743634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing constants.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile constants.py\n",
    "\n",
    "EMBDEDDING_MODEL_PATH = \"/kaggle/input/qwen-3-embedding/transformers/0.6b/1\"\n",
    "MODEL_OUTPUT_PATH = '/kaggle/input/qwen3-8b-embedding'\n",
    "DATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules\"\n",
    "\n",
    "# https://huggingface.co/Qwen/Qwen3-Embedding-0.6B/blob/main/config_sentence_transformers.json\n",
    "EMBEDDING_MODEL_QUERY = \"Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery:\"\n",
    "\n",
    "CLEAN_TEXT = True\n",
    "TOP_K = 2000\n",
    "BATCH_SIZE = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39c7af89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T07:22:29.762113Z",
     "iopub.status.busy": "2025-09-08T07:22:29.761668Z",
     "iopub.status.idle": "2025-09-08T07:22:29.767069Z",
     "shell.execute_reply": "2025-09-08T07:22:29.766284Z"
    },
    "papermill": {
     "duration": 0.010495,
     "end_time": "2025-09-08T07:22:29.768189",
     "exception": false,
     "start_time": "2025-09-08T07:22:29.757694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "import pandas as pd\n",
    "import torch.distributed as dist\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from cleantext import clean\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from constants import CLEAN_TEXT\n",
    "\n",
    "\n",
    "def build_prompt(row):\n",
    "    return f\"\"\"r/{row[\"subreddit\"]}\\nComment: {row[\"body\"]}\"\"\"\n",
    "\n",
    "\n",
    "def cleaner(text):\n",
    "    return clean(\n",
    "        text,\n",
    "        fix_unicode=True,\n",
    "        to_ascii=True,\n",
    "        lower=False,\n",
    "        no_line_breaks=False,\n",
    "        no_urls=True,\n",
    "        no_emails=True,\n",
    "        no_phone_numbers=True,\n",
    "        no_numbers=False,\n",
    "        no_digits=False,\n",
    "        no_currency_symbols=False,\n",
    "        no_punct=False,\n",
    "        replace_with_url=\"<URL>\",\n",
    "        replace_with_email=\"<EMAIL>\",\n",
    "        replace_with_phone_number=\"<PHONE>\",\n",
    "        lang=\"en\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def get_dataframe_to_train(data_path):\n",
    "    train_dataset = pd.read_csv(f\"{data_path}/train.csv\")\n",
    "    test_dataset = pd.read_csv(f\"{data_path}/test.csv\")\n",
    "\n",
    "    flatten = []\n",
    "    flatten.append(train_dataset[[\"body\", \"rule\", \"subreddit\", \"rule_violation\"]])\n",
    "    \n",
    "    for violation_type in [\"positive\", \"negative\"]:\n",
    "        for i in range(1, 3):\n",
    "            sub_dataset = test_dataset[[f\"{violation_type}_example_{i}\", \"rule\", \"subreddit\"]].copy()\n",
    "            sub_dataset = sub_dataset.rename(columns={f\"{violation_type}_example_{i}\": \"body\"})\n",
    "            sub_dataset[\"rule_violation\"] = 1 if violation_type == \"positive\" else 0\n",
    "            flatten.append(sub_dataset)\n",
    "\n",
    "    dataframe = pd.concat(flatten, axis=0)    \n",
    "    dataframe = dataframe.drop_duplicates(ignore_index=True)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def prepare_dataframe(dataframe):\n",
    "    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)\n",
    "\n",
    "    \n",
    "    if CLEAN_TEXT:\n",
    "        tqdm.pandas(desc=\"cleaner\")\n",
    "        dataframe[\"prompt\"] = dataframe[\"prompt\"].progress_apply(cleaner)\n",
    "\n",
    "    if \"rule_violation\" in dataframe.columns:\n",
    "        dataframe[\"rule_violation\"] = dataframe[\"rule_violation\"].map(\n",
    "            {\n",
    "                1: 1,\n",
    "                0: -1,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62eb4483",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T07:22:29.775723Z",
     "iopub.status.busy": "2025-09-08T07:22:29.775502Z",
     "iopub.status.idle": "2025-09-08T07:22:29.781130Z",
     "shell.execute_reply": "2025-09-08T07:22:29.780577Z"
    },
    "papermill": {
     "duration": 0.010455,
     "end_time": "2025-09-08T07:22:29.782121",
     "exception": false,
     "start_time": "2025-09-08T07:22:29.771666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing semantic.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile semantic.py\n",
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import semantic_search, dot_score\n",
    "from tqdm.auto import tqdm\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "\n",
    "from utils import get_dataframe_to_train, prepare_dataframe\n",
    "from constants import DATA_PATH, EMBDEDDING_MODEL_PATH, EMBEDDING_MODEL_QUERY, TOP_K, BATCH_SIZE, MODEL_OUTPUT_PATH\n",
    "\n",
    "\n",
    "\n",
    "def get_scores(test_dataframe):\n",
    "    corpus_dataframe = get_dataframe_to_train(DATA_PATH)\n",
    "    corpus_dataframe = prepare_dataframe(corpus_dataframe)\n",
    "    \n",
    "    # Load base model\n",
    "    model = AutoModelForCausalLM.from_pretrained(EMBDEDDING_MODEL_PATH)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(EMBDEDDING_MODEL_PATH)\n",
    "    \n",
    "    # Load adapter configuration and model\n",
    "    adapter_config = PeftConfig.from_pretrained(MODEL_OUTPUT_PATH)\n",
    "    lora_model = PeftModel.from_pretrained(model, MODEL_OUTPUT_PATH, config=adapter_config)\n",
    "    merged_model = lora_model.merge_and_unload()\n",
    "    tokenizer.save_pretrained(\"Qwen3Emb_Finetuned\")\n",
    "    merged_model.save_pretrained(\"Qwen3Emb_Finetuned\")\n",
    "\n",
    "    # 4. Tạo lại SentenceTransformer từ encoder đã merge\n",
    "    embedding_model = SentenceTransformer(model_name_or_path=\"Qwen3Emb_Finetuned\", device=\"cuda\")\n",
    "\n",
    "    print('Done loading model!')\n",
    "\n",
    "    result = []\n",
    "    for rule in tqdm(test_dataframe[\"rule\"].unique(), desc=f\"Generate scores for each rule\"):\n",
    "        test_dataframe_part = test_dataframe.query(\"rule == @rule\").reset_index(drop=True)\n",
    "        corpus_dataframe_part = corpus_dataframe.query(\"rule == @rule\").reset_index(drop=True)\n",
    "        corpus_dataframe_part = corpus_dataframe_part.reset_index(names=\"row_id\")\n",
    "        \n",
    "        query_embeddings = embedding_model.encode(\n",
    "            sentences=test_dataframe_part[\"prompt\"].tolist(),\n",
    "            prompt=EMBEDDING_MODEL_QUERY,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_tensor=True,\n",
    "            device=\"cuda\",\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "        document_embeddings = embedding_model.encode(\n",
    "            sentences=corpus_dataframe_part[\"prompt\"].tolist(),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_tensor=True,\n",
    "            device=\"cuda\",\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "        test_dataframe_part[\"semantic\"] = semantic_search(\n",
    "            query_embeddings,\n",
    "            document_embeddings,\n",
    "            top_k=TOP_K,\n",
    "            score_function=dot_score,\n",
    "        )\n",
    "        def get_score(semantic):\n",
    "            semantic = pd.DataFrame(semantic)\n",
    "            semantic = semantic.merge(\n",
    "                corpus_dataframe_part[[\"row_id\", \"rule_violation\"]],\n",
    "                how=\"left\",\n",
    "                left_on=\"corpus_id\",\n",
    "                right_on=\"row_id\",\n",
    "            )\n",
    "            semantic[\"score\"] = semantic[\"score\"]*semantic[\"rule_violation\"]\n",
    "            return semantic[\"score\"].sum()\n",
    "            \n",
    "        tqdm.pandas(desc=f\"Add label for {rule=}\")\n",
    "        test_dataframe_part[\"rule_violation\"] = test_dataframe_part[\"semantic\"].progress_apply(get_score)\n",
    "        result.append(test_dataframe_part[[\"row_id\", \"rule_violation\"]].copy())\n",
    "        \n",
    "    submission = pd.concat(result, axis=0)\n",
    "    return submission\n",
    "\n",
    "\n",
    "def generate_submission():\n",
    "    test_dataframe = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n",
    "    test_dataframe = prepare_dataframe(test_dataframe)\n",
    "    \n",
    "    submission = get_scores(test_dataframe)\n",
    "    submission = test_dataframe[[\"row_id\"]].merge(submission, on=\"row_id\", how=\"left\")\n",
    "    submission.to_csv(\"submission_qwen3.csv\", index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_submission()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96123a15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T07:22:29.789595Z",
     "iopub.status.busy": "2025-09-08T07:22:29.789414Z",
     "iopub.status.idle": "2025-09-08T07:23:01.349778Z",
     "shell.execute_reply": "2025-09-08T07:23:01.348859Z"
    },
    "papermill": {
     "duration": 31.565813,
     "end_time": "2025-09-08T07:23:01.351346",
     "exception": false,
     "start_time": "2025-09-08T07:22:29.785533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-08 07:22:45.509336: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1757316165.672225      92 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1757316165.721885      92 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/tmp/semantic.py\", line 9, in <module>\r\n",
      "    from utils import get_dataframe_to_train, prepare_dataframe\r\n",
      "  File \"/tmp/utils.py\", line 5, in <module>\r\n",
      "    from cleantext import clean\r\n",
      "ModuleNotFoundError: No module named 'cleantext'\r\n"
     ]
    }
   ],
   "source": [
    "!python semantic.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584a06f0",
   "metadata": {
    "papermill": {
     "duration": 0.003441,
     "end_time": "2025-09-08T07:23:01.358820",
     "exception": false,
     "start_time": "2025-09-08T07:23:01.355379",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Llama 3.1 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdda6fc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T07:23:01.367197Z",
     "iopub.status.busy": "2025-09-08T07:23:01.366893Z",
     "iopub.status.idle": "2025-09-08T07:23:01.373787Z",
     "shell.execute_reply": "2025-09-08T07:23:01.373200Z"
    },
    "papermill": {
     "duration": 0.012401,
     "end_time": "2025-09-08T07:23:01.374778",
     "exception": false,
     "start_time": "2025-09-08T07:23:01.362377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing llama.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile llama.py\n",
    "import os, math, numpy as np\n",
    "#指定程序可以使用的GPU的序号\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "sys_prompt = '''You are given a comment on reddit and a rule. Your task is to classify whether the comment violates the rule. Only respond Yes/No.'''\n",
    "\n",
    "#将一个包含多段对话（conversation）的数据集，\n",
    "#批量转换成符合特定聊天模型（Chat Model）要求的、格式化好的纯文本字符串。\n",
    "def formatting(dataset):\n",
    "    texts = []\n",
    "    for i in range(len(dataset)):\n",
    "        texts.append(tokenizer.apply_chat_template(dataset[i], tokenize=False, add_generation_prompt=False))\n",
    "    return texts\n",
    "\n",
    "\n",
    "#提示模板 (prompt template)，\n",
    "#它的作用是为语言模型（LLM）构建一个结构化、带示例的输入，以引导它完成一项特定的任务。\n",
    "template = \"\"\"\n",
    "Subreddit: r/{subreddit}\n",
    "Rule: {rule}\n",
    "Examples:\n",
    "1) {positive_example_1}\n",
    "Violation: Yes\n",
    "\n",
    "2) {negative_example_1}\n",
    "Violation: No\n",
    "\n",
    "3) {negative_example_2}\n",
    "Violation: No\n",
    "\n",
    "4) {positive_example_2}\n",
    "Violation: Yes\n",
    "Comment:\n",
    "{body}\n",
    "Violation: \"\"\"\n",
    "\n",
    "dataset = []\n",
    "#对可循环对象test.iterrows的每一行进行循环index为序列号，row为行内容\n",
    "for index,row in test.iterrows():\n",
    "    \n",
    "    formatted_sample = [\n",
    "        {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": sys_prompt\n",
    "    },\n",
    "       {\n",
    "           \"role\": \"user\",\n",
    "           #.format() 是 Python 字符串的一个方法，用于填充字符串中的占位符（那些用 {} 括起来的部分）。\n",
    "           \"content\": template.format(\n",
    "               rule = row.rule,\n",
    "               subreddit = row.subreddit,\n",
    "               body = row.body,\n",
    "               positive_example_1 = row.positive_example_1,\n",
    "               negative_example_1 = row.negative_example_1,\n",
    "               positive_example_2 = row.positive_example_2,\n",
    "               negative_example_2 = row.negative_example_2\n",
    "           )\n",
    "       }]\n",
    "    \n",
    "    dataset.append( formatted_sample )\n",
    "\n",
    "#将多端对话变成一个字符串\n",
    "all_prompts = formatting(dataset)\n",
    "\n",
    "#这里是在创建示例tokenizer是作为参数传递进去的。\n",
    "logits_processors = [DigitLogitsProcessor(tokenizer)]\n",
    "#这里是正式生成结果\n",
    "responses = llm.generate(\n",
    "    all_prompts,\n",
    "    vllm.SamplingParams(\n",
    "        n=1,  # 控制返回答案的数量。如果你想让模型对同一个问题给出多种不同的回答，可以把 n 调高，比如 n=3 就会返回三个独立的答案序列。.\n",
    "        top_p=0.9,  # 控制模型在生成下一个词时考虑的词汇范围。模型会先按概率高低对所有可能的下一个词进行排序，然后从高到低累加它们的概率，直到总和达到 0.9 (90%) 为止。\n",
    "        temperature=0,  # rtemperature=0: 意味着完全没有随机性。模型在每一步都会选择概率最高的那个词（贪心搜索），结果是完全确定的。\n",
    "        seed=777, # Seed for reprodicibility\n",
    "        skip_special_tokens=True,  # 在最终输出中跳过（不显示）特殊标记。\n",
    "        max_tokens=1,  # Maximum number of tokens to generate per output sequence.\n",
    "        logits_processors=logits_processors,\n",
    "        #返回每个生成词元（token）的对数概率，以及最有可能的 2 个词元的信息。\n",
    "        logprobs = 2\n",
    "    ),\n",
    "    use_tqdm = True\n",
    ")\n",
    "\n",
    "results = []\n",
    "errors = 0\n",
    "#enumerate() 函数让你在循环时可以同时获得索引 i（0, 1, 2...）和响应对象 response。\n",
    "for i,response in enumerate(responses):\n",
    "    try:\n",
    "        #第一个结果的第一个生成的token以及最高概率的两个概率\n",
    "        #即Yes和No的概率，属于[0,1.]\n",
    "        x = response.outputs[0].logprobs[0]\n",
    "        logprobs = []\n",
    "        for k in KEEP:\n",
    "            if k in x:\n",
    "                logprobs.append( math.exp(x[k].logprob) )\n",
    "            else:\n",
    "                logprobs.append( 0 )\n",
    "                print(f\"bad logits {i}\")\n",
    "        #转换为np数组\n",
    "        logprobs = np.array( logprobs )\n",
    "        #概率归一化\n",
    "        logprobs /= logprobs.sum()\n",
    "        results.append( logprobs )\n",
    "    except:\n",
    "        #print(f\"error {i}\")\n",
    "        #数字后面加.代表转换为浮点类型\n",
    "        results.append( np.array([1/2., 1/2.]) )\n",
    "        errors += 1\n",
    "        \n",
    "print(f\"There were {errors} inference errors out of {i+1} inferences\")\n",
    "#将多个np数组组成的列表变成一个二维的np数组或np矩阵\n",
    "results = np.vstack(results)\n",
    "\n",
    "#仅取违规的概率，即Yes的概率\n",
    "probs = [x[1] for x in results]\n",
    "sub['rule_violation'] = probs\n",
    "sub.to_csv('submission_llama.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "476d9113",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T07:23:01.382941Z",
     "iopub.status.busy": "2025-09-08T07:23:01.382519Z",
     "iopub.status.idle": "2025-09-08T07:23:01.684985Z",
     "shell.execute_reply": "2025-09-08T07:23:01.684260Z"
    },
    "papermill": {
     "duration": 0.307906,
     "end_time": "2025-09-08T07:23:01.686329",
     "exception": false,
     "start_time": "2025-09-08T07:23:01.378423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/tmp/llama.py\", line 38, in <module>\r\n",
      "    for index,row in test.iterrows():\r\n",
      "                     ^^^^\r\n",
      "NameError: name 'test' is not defined\r\n"
     ]
    }
   ],
   "source": [
    "!python llama.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ece2ec7",
   "metadata": {
    "papermill": {
     "duration": 0.003731,
     "end_time": "2025-09-08T07:23:01.694139",
     "exception": false,
     "start_time": "2025-09-08T07:23:01.690408",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. ENSEMBLE RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf719f97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T07:23:01.702936Z",
     "iopub.status.busy": "2025-09-08T07:23:01.702210Z",
     "iopub.status.idle": "2025-09-08T07:23:01.707485Z",
     "shell.execute_reply": "2025-09-08T07:23:01.706765Z"
    },
    "papermill": {
     "duration": 0.010671,
     "end_time": "2025-09-08T07:23:01.708499",
     "exception": false,
     "start_time": "2025-09-08T07:23:01.697828",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "q = pd.read_csv('submission_qwen.csv')\n",
    "l = pd.read_csv('submission_qwen3.csv')\n",
    "m = pd.read_csv('submission_llama.csv')\n",
    "\n",
    "rq = q['rule_violation'].rank(method='average') / (len(q)+1)\n",
    "rl = l['rule_violation'].rank(method='average') / (len(l)+1)\n",
    "rm = m['rule_violation'].rank(method='average') / (len(m)+1)\n",
    "\n",
    "blend = 0.5*rq + 0.4*rl + 0.1*rm   # or tune the rank-weights with a tiny grid using OOF\n",
    "q['rule_violation'] = blend\n",
    "q.to_csv('/kaggle/working/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "360d4886",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T07:23:01.717018Z",
     "iopub.status.busy": "2025-09-08T07:23:01.716573Z",
     "iopub.status.idle": "2025-09-08T07:23:02.335755Z",
     "shell.execute_reply": "2025-09-08T07:23:02.335015Z"
    },
    "papermill": {
     "duration": 0.625331,
     "end_time": "2025-09-08T07:23:02.337640",
     "exception": false,
     "start_time": "2025-09-08T07:23:01.712309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/tmp/main.py\", line 4, in <module>\r\n",
      "    q = pd.read_csv('submission_qwen.csv')\r\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\", line 1026, in read_csv\r\n",
      "    return _read(filepath_or_buffer, kwds)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\", line 620, in _read\r\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\r\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\", line 1620, in __init__\r\n",
      "    self._engine = self._make_engine(f, self.engine)\r\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\", line 1880, in _make_engine\r\n",
      "    self.handles = get_handle(\r\n",
      "                   ^^^^^^^^^^^\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\", line 873, in get_handle\r\n",
      "    handle = open(\r\n",
      "             ^^^^^\r\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'submission_qwen.csv'\r\n"
     ]
    }
   ],
   "source": [
    "!python main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58ffa166",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T07:23:02.347127Z",
     "iopub.status.busy": "2025-09-08T07:23:02.346588Z",
     "iopub.status.idle": "2025-09-08T07:23:02.379732Z",
     "shell.execute_reply": "2025-09-08T07:23:02.378812Z"
    },
    "papermill": {
     "duration": 0.038993,
     "end_time": "2025-09-08T07:23:02.380938",
     "exception": false,
     "start_time": "2025-09-08T07:23:02.341945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exists: True\n",
      "top files: ['adapter_model.safetensors', 'merges.txt', 'training_args.bin', 'adapter_config.json', 'README.md', 'tokenizer.json', 'vocab.json', 'tokenizer_config.json', 'chat_template.jinja', 'special_tokens_map.json', 'added_tokens.json']\n",
      "--- README.md ---\n",
      "---\n",
      "base_model: Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4\n",
      "library_name: peft\n",
      "pipeline_tag: text-generation\n",
      "tags:\n",
      "- base_model:adapter:Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4\n",
      "- lora\n",
      "- transformers\n",
      "---\n",
      "\n",
      "# Model Card for Model ID\n",
      "\n",
      "<!-- Provide a quick summary of what the model is/does. -->\n",
      "\n",
      "\n",
      "\n",
      "## Model Details\n",
      "\n",
      "### Model Description\n",
      "\n",
      "<!-- Provide a longer summary of what this model is. -->\n",
      "\n",
      "\n",
      "\n",
      "- **Developed by:** [More Information Needed]\n",
      "- **Funded by [optional]:** [More Information Needed]\n",
      "- **Shared by [optional]:** [More Information Needed]\n",
      "- **Model type:** [More Information Needed]\n",
      "- **Language(s) (NLP):** [More Information Needed]\n",
      "- **License:** [More Information Needed]\n",
      "- **Finetuned from model [optional]:** [More Information Needed]\n",
      "\n",
      "### Model Sources [optional]\n",
      "\n",
      "<!-- Provide the basic links for the model. -->\n",
      "\n",
      "- **Repository:** [More Information Needed]\n",
      "- **Paper [optional]:** [More Information Needed]\n",
      "- **Demo [optional]:** [More Information Needed]\n",
      "\n",
      "## Uses\n",
      "\n",
      "<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n",
      "\n",
      "### Direct Use\n",
      "\n",
      "<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\n",
      "\n",
      "[More Information Needed]\n",
      "\n",
      "### Downstream Use [optional]\n",
      "\n",
      "<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->\n",
      "\n",
      "[More Information Needed]\n",
      "\n",
      "### Out-of-Scope Use\n",
      "\n",
      "<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\n",
      "\n",
      "[More Information Needed]\n",
      "\n",
      "## Bias, Risks, and Limitations\n",
      "\n",
      "<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n",
      "\n",
      "[More Information Needed]\n",
      "\n",
      "### Recommendations\n",
      "\n",
      "<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\n",
      "\n",
      "Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More infor\n",
      "--- adapter_config.json ---\n",
      "{\n",
      "  \"alpha_pattern\": {},\n",
      "  \"auto_mapping\": null,\n",
      "  \"base_model_name_or_path\": \"Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4\",\n",
      "  \"bias\": \"none\",\n",
      "  \"corda_config\": null,\n",
      "  \"eva_config\": null,\n",
      "  \"exclude_modules\": null,\n",
      "  \"fan_in_fan_out\": false,\n",
      "  \"inference_mode\": true,\n",
      "  \"init_lora_weights\": true,\n",
      "  \"layer_replication\": null,\n",
      "  \"layers_pattern\": null,\n",
      "  \"layers_to_transform\": null,\n",
      "  \"loftq_config\": {},\n",
      "  \"lora_alpha\": 16,\n",
      "  \"lora_bias\": false,\n",
      "  \"lora_dropout\": 0.05,\n",
      "  \"megatron_config\": null,\n",
      "  \"megatron_core\": \"megatron.core\",\n",
      "  \"modules_to_save\": null,\n",
      "  \"peft_type\": \"LORA\",\n",
      "  \"qalora_group_size\": 16,\n",
      "  \"r\": 16,\n",
      "  \"rank_pattern\": {},\n",
      "  \"revision\": null,\n",
      "  \"target_modules\": [\n",
      "    \"v_proj\",\n",
      "    \"o_proj\",\n",
      "    \"up_proj\",\n",
      "    \"down_proj\",\n",
      "    \"gate_proj\",\n",
      "    \"k_proj\",\n",
      "    \"q_proj\"\n",
      "  ],\n",
      "  \"target_parameters\": null,\n",
      "  \"task_type\": \"CAUSAL_LM\",\n",
      "  \"trainable_token_indices\": null,\n",
      "  \"use_dora\": false,\n",
      "  \"use_qalora\": false,\n",
      "  \"use_rslora\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "p = \"/kaggle/input/qwen2-5-32b-gptq-int4-batch4-full\"\n",
    "print(\"exists:\", os.path.exists(p))\n",
    "print(\"top files:\", os.listdir(p))\n",
    "for name in [\"README.md\",\"README\",\"LICENSE\",\"license\",\"config.json\",\"adapter_config.json\"]:\n",
    "    fp = os.path.join(p, name)\n",
    "    if os.path.exists(fp):\n",
    "        print(f\"--- {name} ---\")\n",
    "        print(open(fp, \"r\", encoding=\"utf-8\", errors=\"ignore\").read()[:2000])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13121456,
     "sourceId": 94635,
     "sourceType": "competition"
    },
    {
     "datasetId": 8044304,
     "sourceId": 12726948,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8067935,
     "sourceId": 12762469,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 252850661,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 252853424,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 257213453,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 121030,
     "modelInstanceId": 97853,
     "sourceId": 116441,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 368803,
     "modelInstanceId": 347541,
     "sourceId": 426330,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 39.661194,
   "end_time": "2025-09-08T07:23:02.600647",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-08T07:22:22.939453",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
